# ğŸš€ Time Series Forecasting com TinyTimeMixer (TTM) - IBM Granite

## ğŸ“‹ VisÃ£o Geral

Este projeto implementa um modelo de previsÃ£o de sÃ©ries temporais utilizando o **TinyTimeMixer (TTM)** da IBM Granite para prever vendas e faturamento por produto e UF. O modelo utiliza tÃ©cnicas avanÃ§adas de deep learning para sÃ©ries temporais, incluindo fine-tuning seletivo e preprocessamento automatizado.

### ğŸ¯ Objetivo

Desenvolver um sistema de previsÃ£o capaz de:
- Prever vendas e faturamento para diferentes produtos e UFs
- Utilizar janelas de contexto de 2 anos (104 semanas) para previsÃµes de 6 meses (26 semanas)
- Aplicar fine-tuning eficiente em modelo prÃ©-treinado
- Garantir reprodutibilidade e robustez nas previsÃµes

## ğŸ—ï¸ Arquitetura do Projeto

### Componentes Principais

1. **Carregamento e PreparaÃ§Ã£o dos Dados**
   - Carregamento do dataset de vendas/faturamento
   - GeraÃ§Ã£o de dados sintÃ©ticos para demonstraÃ§Ã£o
   - Resampling para frequÃªncia semanal consistente

2. **PrÃ©-processamento**
   - NormalizaÃ§Ã£o/escalonamento por grupo produto-UF
   - CodificaÃ§Ã£o de variÃ¡veis categÃ³ricas
   - DivisÃ£o estratificada preservando combinaÃ§Ãµes

3. **ConfiguraÃ§Ã£o do Modelo TTM**
   - Carregamento do modelo prÃ©-treinado IBM Granite
   - ConfiguraÃ§Ã£o personalizada para o dataset
   - Fine-tuning seletivo (apenas camadas fc1/fc2)

4. **Treinamento**
   - OtimizaÃ§Ã£o AdamW com OneCycleLR scheduler
   - Early stopping com paciÃªncia de 15 Ã©pocas
   - Monitoramento automÃ¡tico de mÃ©tricas

5. **Salvamento**
   - Modelo final otimizado
   - Preprocessador para inferÃªncia
   - Logs de treinamento e mÃ©tricas

## ğŸ› ï¸ ConfiguraÃ§Ã£o do Ambiente

### PrÃ©-requisitos

- Python 3.8+
- CUDA 11.8+ (opcional, para GPU)
- 16GB+ RAM recomendado
- 10GB+ espaÃ§o em disco

### InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**
```bash
git clone url-do-repositorio>](https://github.com/renatobarrosai/ttm-r2.1-challenge.git
cd treinamento_semana
```

2. **Crie um ambiente virtual:**
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\\Scripts\\activate  # Windows
```

3. **Instale as dependÃªncias:**
```bash
pip install -r requirements.txt
```

4. **Instale o IBM Granite TSFM:**
```bash
pip install "granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.22"
```

## ğŸ“Š Estrutura dos Dados

### Formato Esperado

O arquivo `./dados/db_tratado-w.csv` deve conter:

| Coluna | Tipo | DescriÃ§Ã£o |
|--------|------|-----------|
| `date` | datetime | Data no formato YYYY-MM-DD |
| `produto_cat` | int | Categoria do produto (codificada) |
| `uf_cat` | int | UF categorizada (codificada) |
| `vendas` | float | Volume de vendas |
| `faturamento` | float | Valor do faturamento |

### Exemplo de Dados

```csv
date,produto_cat,uf_cat,vendas,faturamento
2023-01-01,1,2,300,1000.0
2023-01-08,1,2,310,1100.0
2023-01-15,1,2,305,1050.0
```

## ğŸš€ ExecuÃ§Ã£o

### Treinamento do Modelo

```bash
python ttm_model.py
```

### Principais ParÃ¢metros ConfigurÃ¡veis

```python
# ConfiguraÃ§Ãµes temporais
CONTEXT_LENGTH = 104        # 2 anos de histÃ³rico semanal
PREDICTION_LENGTH = 26      # 6 meses de previsÃ£o

# HiperparÃ¢metros de treinamento
LEARNING_RATE = 5e-4        # Taxa de aprendizado
NUM_TRAIN_EPOCHS = 100      # MÃ¡ximo de Ã©pocas
BATCH_SIZE = 2              # Batch size para treinamento
```

## ğŸ“ˆ Monitoramento

### MÃ©tricas Acompanhadas

- **Loss de Treinamento**: FunÃ§Ã£o de perda durante treinamento
- **Loss de ValidaÃ§Ã£o**: FunÃ§Ã£o de perda no conjunto de validaÃ§Ã£o
- **Early Stopping**: Controle automÃ¡tico de overfitting
- **ParÃ¢metros TreinÃ¡veis**: Monitoramento de eficiÃªncia

### Logs e Outputs

```
./results_ttm_model/
â”œâ”€â”€ logs/                   # TensorBoard logs
â”œâ”€â”€ checkpoint-*/           # Checkpoints intermediÃ¡rios
â””â”€â”€ pytorch_model.bin       # Modelo final

./final_ttm_model/
â”œâ”€â”€ config.json            # ConfiguraÃ§Ã£o do modelo
â”œâ”€â”€ pytorch_model.bin      # Pesos do modelo
â””â”€â”€ preprocessor.pkl       # Preprocessador serializado
```

## ğŸ”§ Funcionalidades Principais

### 1. Carregamento Inteligente de Dados
- DetecÃ§Ã£o automÃ¡tica de arquivo de dados
- GeraÃ§Ã£o de dados sintÃ©ticos para demonstraÃ§Ã£o
- ValidaÃ§Ã£o de formato e consistÃªncia

### 2. Preprocessamento Robusto
- Resampling automÃ¡tico para frequÃªncia semanal
- NormalizaÃ§Ã£o por grupo produto-UF
- DivisÃ£o estratificada sem vazamento de dados

### 3. Fine-tuning Eficiente
- Congelamento seletivo de camadas
- ReduÃ§Ã£o de ~95% dos parÃ¢metros treinÃ¡veis
- OtimizaÃ§Ã£o focada em camadas especÃ­ficas

### 4. Treinamento Otimizado
- Early stopping inteligente
- Scheduler de learning rate adaptativo
- Salvamento automÃ¡tico do melhor modelo

## ğŸ“‹ Estrutura do CÃ³digo

```
ttm_model.py
â”œâ”€â”€ SEÃ‡ÃƒO 1: Carregamento e ConfiguraÃ§Ã£o Inicial
â”œâ”€â”€ SEÃ‡ÃƒO 2: PrÃ©-processamento e PreparaÃ§Ã£o
â”œâ”€â”€ SEÃ‡ÃƒO 3: DivisÃ£o dos Dados e CriaÃ§Ã£o dos Datasets
â”œâ”€â”€ SEÃ‡ÃƒO 4: ConfiguraÃ§Ã£o e InicializaÃ§Ã£o do Modelo
â””â”€â”€ SEÃ‡ÃƒO 5: ConfiguraÃ§Ã£o e ExecuÃ§Ã£o do Treinamento
```

### Principais FunÃ§Ãµes

| FunÃ§Ã£o | DescriÃ§Ã£o |
|--------|-----------|
| `load_data()` | Carrega dados ou gera exemplos sintÃ©ticos |
| `resample_data_to_weekly()` | Aplica resampling semanal consistente |
| `split_data_by_combinations()` | Divide dados preservando combinaÃ§Ãµes |
| `create_ttm_config()` | Configura modelo TTM personalizado |
| `setup_selective_fine_tuning()` | Configura fine-tuning seletivo |
| `train_ttm_model()` | Executa treinamento completo |

## ğŸ¯ Resultados Esperados

### Performance
- **ReduÃ§Ã£o de ParÃ¢metros**: ~95% menos parÃ¢metros treinÃ¡veis
- **Tempo de Treinamento**: Significativamente reduzido vs. full fine-tuning
- **GeneralizaÃ§Ã£o**: Melhor performance em dados nÃ£o vistos

### Outputs do Modelo
- PrevisÃµes de vendas para prÃ³ximas 26 semanas
- PrevisÃµes de faturamento para prÃ³ximas 26 semanas
- Intervalos de confianÃ§a (quando disponÃ­veis)
- MÃ©tricas de performance por produto-UF

## ğŸ” Troubleshooting

### Problemas Comuns

1. **Erro de MemÃ³ria GPU**
   ```python
   # Reduzir batch size
   PER_DEVICE_TRAIN_BATCH_SIZE = 1
   ```

2. **Arquivo de Dados NÃ£o Encontrado**
   - O cÃ³digo automaticamente gera dados sintÃ©ticos
   - Verifique o caminho: `./dados/db_tratado-w.csv`

3. **DependÃªncias NÃ£o Instaladas**
   ```bash
   pip install -r requirements.txt
   pip install "granite-tsfm[notebooks] @ git+https://github.com/ibm-granite/granite-tsfm.git@v0.2.22"
   ```

## ğŸ“š ReferÃªncias

- [IBM Granite Time Series Foundation Models](https://github.com/ibm-granite/granite-tsfm)
- [TinyTimeMixer Paper](https://arxiv.org/abs/2401.03955)
- [Hugging Face Transformers](https://huggingface.co/docs/transformers)
- [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/)

## ğŸ¤ ContribuiÃ§Ã£o

1. Fork o projeto
2. Crie uma branch para sua feature (`git checkout -b feature/AmazingFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Add some AmazingFeature'`)
4. Push para a branch (`git push origin feature/AmazingFeature`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo `LICENSE` para mais detalhes.

## ğŸ‘¥ Autores

- **Renato Barros** - *Desenvolvimento inicial* - [renatobarrosai](https://github.com/seuusername](https://github.com/renatobarrosai/)

## ğŸ™ Agradecimentos

- IBM Research pela disponibilizaÃ§Ã£o dos modelos Granite
- Hugging Face pela infraestrutura de modelos
- Comunidade open-source de Time Series Forecasting

---

**âš¡ Para mais informaÃ§Ãµes ou suporte, abra uma issue no repositÃ³rio!**
